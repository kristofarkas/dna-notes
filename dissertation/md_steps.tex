\subsection{Main steps in a molecule dynamics simulation protocol}

Every system that is investigated via simulation method required specific care, but there are four steps that are present in most molecular dynamcs workflows. They are not hard written rules, but following these steps assures that the system is correctly equilibrated and sampled during simulations, and also minimizes the chance that the energy of the system gets out of control.

The four steps are: system preparation, minimisation, equilibration and production.

\subsubsection{Minimisation}

During minimization or relaxation, we want to find a local minimum of the biological system so that during the start of the molecule dynamics simulation the system does not end up in an unstable state (sometimes termed ``blow up'') This can happen due to forces on certain atoms being too large at the start, and they were to move too much during the first couple of timesteps of the simulation. To achieve this standard minimisation algorithms are employed, sich as steepest descent or the fire minimiser. 

\subsubsection{Assignment of velocities}

As discuss before, minimisation takes the system to a state where we can start the numerical integration of the equation of motion without the possibility that any subsequent displacement will be too large or physically unrealistic,  however, minimisation only takes into account the positions of every particle, and does not change the velocities, therefore we have to set the velocity vectors of every particle separately at the start of the simulation. Thus, to set the starting velocities one usually assigns random initial velocities to atoms in a way such that the correct Maxwell-Boltzmann distribution at the desired temperature is achieved as a starting point. The actual assignment process is typically unimportant, as the Maxwell-Boltzmann distribution will quickly arise naturally from the equations of motion. The last particle in the simulation box is typically given a velocity such that the centre of mass momentum is zero, and the simulation box itself as a whole does not drift with time. This is because in Newtonian dynamics the momentum of the centre of mass is conserved. 

Single simulation trajectories are somewhat meaningless, and in most cases, we want to run a set of independent simulations, of different replicase or realisations of the a particular system. This is to assess the error on the particular observable, and collect statistical descriptors like the spread, mean and standard deviation, or help understand how the starting structure effect the results. 
It is important to note, that even the smallest difference in the inital conditions of the system (for example the position of a single atom, or magnitude of velocity) can lead to completely divergent results after some simulation time, meaning that simply running different simulations starting with different initial velocities will lead to completely different time evolution over a long enough simulation. An even better approach to generating different initial replicas, is to start from completely different conformations such as different conformations of the molecules simulations, different binding pose in a receptor ligand simulation, as this will lead to divergence right from the beginning of the simulation.

\subsubsection{Equilibration}

In order to compare the data generated by simulation to experiment, we usually want to bring the simulation conditions to experimentally comparable values (including, temperature, pressure, energy) which requires simulations in a specific ensemble (NVE, NPT, etc.). Data collected should not be biased by the initial conditions on the simulation either. Therefore we need to run the simulations for some period of time until the the system is at the appropriate state, as well as relaxed from any metastable state introduced by the starting state of the simulation. In other words we are interested in sampling the most likely, equilibrium state of the system in the given ensemble. In case of biomolecules it is important to bring them to a configuration that is relevant and could take a long simulation time to reach such state.

The first step in the equilibration phase is to bring the system to the desired thermodynamic state. By thermodynamic state we mean a given temperature are pressure. Assigning velocities with the correct distribution does set the inital temperature, but the thermostat will still have to partition the kinetic and potential energy of the system correctly. To this end, a thermostated equilibration period is run before collecting data even in the case of the NVE ensemble. To monitor the equilibration phase, we look at the temperature, pressure, kinetic and potential energy of the system over time and monitor their fluctuations. A simulation is siad to have equilibrated when these parameters reach a stead state and only fluctuate around that value with minial drift. This definition while not perfect, is a good measure of equilibration of a system to a given pressure, temperature, simulation box (i.e. volume) or energy.

Other properties of the system of interest should not change too during simulation. This is harder to monitor then the thermodynamic properties discussed above. Even at equilibrium the system might undergo slow fluctuations especially if it has slow degrees of freedom, but the values should stay around a specific value. The bio-molecular system it is common to investigate the root mean squared deviation of the particles over time. and potentially other properties like the number of hydrogen bonds between the biomolecules present and water, as these may be slower to equilibrate than system-wide properties like the temperature and pressure.

Once the energy component and other properties fluctuate around constant values with minimal or no drift we have reach the end of equilibration. If some of the properties of interest exhibit a systematic drift with time, that the system has probably not equilibrated sufficiently. 

The target ensemble introduces a difference in the equilibration protocol.  In the NVE ensemble the thermostat should be removed and frame of the system should be selected that is as close to the desired kinetic and potential energy as possible. The frame containing the positions and velocities of every particle is used to start the production simulation at the correct temperature. This is because the temperature fluctuates over the simulation when coupled to a thermostat.

If the target is the NPT ensemble (that is the most common for biological simulation) then the system should first be relax to the desired temperature in the the NVT ensemble, then the barostat turned on to relax the simulation box to the desired pressure.

\subsubsection{Production}

Once the system has equilibrated we can step into the final stage where we collect the data about the system. This phase of the workflow is called production. The main difference between the end of the equilibration and the production run is that in production we keep and analyse the data produce as opposed to discarding it. Production always comes after a rigorous equilibration phase in the correct ensemble, and it should never be collected after a sudden change of any of the properties of the simulation, like the simulation box size, temperature, pressure (unless this is the goal of the simulation).

If the ensemble changes from equilibration to production, then it is advised to discard the initial data collected even in the production simulation (for example switching from NPT to NVT) with the usual considerations about equilibrium still applicable here.

Analysis of the production work required special care as some of the observable are not trivial to calculate, and the error on these values must be correctly estimated. Usually, analysis involves calculating the expectation values of certain observables, with the important consideration that these values are converged, meaning that they no longer depend on the further length of the simulation (or replica of simulations) or on the initial conditions that the simulation was started from. This is very much related to the equilibration discussed above, with the difference being what is the subject at hand. Depending on the time to relaxation, it is common to realise that the system was not equilibrated after analysis of the production run.

A key consideration is the storage and frequency of storage of the simulated data. Storing data very frequently (for example storing at every timestep) can be tempting, but limits in the physical availability of storage space is usually a limiting factor, but also the additional information contained in snapshots close together diminished drastically as the time separation become small. In particular in molecular dynamics simulations consecutive frames are correlated, so storing and analysing each frame is essentially redundant and adds no value or insight to the calculation of a specific observable. Therefore, storing data more frequently than the autocorrelation time is not necessary. Disk space should also be taken into consideration. Depending on the size of the system (the number of particles) the size of a single frame in a trajectory can be large. But even if disk space is not an issue, storing snapshots with an autocorrelation frequency should be enough.

There are various strategies to reduce size of the data. One can store reduced precision of the coordinates, or chose to save with different frequency the component of interest. For example because energy is much smaller to store, it can be storied with a higher frequency then trajectory coordinates. This depends on the analysis at hand.

